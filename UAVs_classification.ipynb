{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8391052,"sourceType":"datasetVersion","datasetId":4991220}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nfrom tqdm import tqdm\nimport os\nfrom sklearn.preprocessing import LabelEncoder\nimport torchvision.transforms as transforms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport zipfile\nfrom torchvision.models import resnet50, ResNet50_Weights\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom transformers import MobileNetV1Config, MobileNetV1Model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/uavsdata/UAVs'\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_folders = os.listdir(path)\ndata = []  \nlabels = []\nfor class_folder in class_folders:\n    class_path = os.path.join(path, class_folder)\n    if os.path.isdir(class_path):  \n        file_list = os.listdir(class_path)\n        for img_name in tqdm(file_list):\n            img_path = os.path.join(class_path, img_name)\n            label = class_folder \n            try:\n                img = cv2.imread(img_path)  \n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  \n                img = cv2.resize(img, (224, 224))  \n                data.append(img)  \n                labels.append(label) \n            except Exception as e:\n                pass\n\ndata = np.array(data)\nlabels = np.array(labels)\n\nprint(\"Data shape:\", data.shape)\nprint(\"Labels shape:\", labels.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_labels = np.unique(labels)\nfig, axs = plt.subplots(len(unique_labels), 1, figsize=(8, 8 * len(unique_labels)))\n\nfor i, label in enumerate(unique_labels):\n    idx = np.where(labels == label)[0][0]\n    axs[i].imshow(data[idx])\n    axs[i].set_title(f\"Class: {label}\\nImage Size: {data[idx].shape}\")\n    axs[i].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_of_classes = LabelEncoder()\nlabels = num_of_classes.fit_transform(labels)\nprint(f\"Total Number of Classes: {len(num_of_classes.classes_)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(data, labels, train_size=0.7)\nprint(f\"X_train: {len(x_train)}  X_test: {len(x_test)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, data, labels, transform=None):\n        self.data = data\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        image = self.data[idx]\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean = [0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=32\nepoch = 20\n\ntrain_dataset = CustomDataset(x_train, y_train, transform)\ntest_dataset = CustomDataset(x_test, y_test, transform)\n\ntrainloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntestloader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r_cnn = resnet50(weights=ResNet50_Weights)\nalexnet_pretrained = models.alexnet(pretrained=True)\ngoogle_cnn = models.googlenet(pretrained=True)\nconfiguration = MobileNetV1Config()\npretrained_mobilenet = MobileNetV1Model(configuration)\n\nr_cnn.to(device)\npretrained_mobilenet.to(device)\nalexnet_pretrained.to(device)\ngoogle_cnn.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_validate_model(model, train_loader, test_loader, criterion, optimizer, type_, epochs=10):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    train_losses = []\n    test_losses = []\n    train_accuracies = []\n    test_accuracies = []\n    train_predicted = []\n    train_target = []\n    test_predicted = []\n    test_target = []\n    train_precisions = []\n    test_precisions = []\n    train_recalls = []\n    test_recalls = []\n    train_f1scores = []\n    test_f1scores = []\n    \n    early_stopping_iteration = 0\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in trainloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            if type_ == 2:\n                loss = criterion(outputs.logits, labels) \n            else:\n                loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            train_predicted.extend(predicted.cpu().numpy())\n            train_target.extend(labels.cpu().numpy())\n\n        train_loss = running_loss / len(trainloader)\n        train_losses.append(train_loss)\n\n\n        model.eval()\n        correct = 0\n        total = 0\n        test_loss = 0.0\n        with torch.no_grad():\n            for inputs, labels in testloader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                if type_ == 2:\n                    loss = criterion(outputs.logits, labels) \n                else:\n                    loss = criterion(outputs, labels)\n                test_loss += loss.item()\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n                test_predicted.extend(predicted.cpu().numpy())\n                test_target.extend(labels.cpu().numpy())\n\n        test_loss /= len(testloader)\n        test_losses.append(test_loss)\n\n        train_accuracy = accuracy_score(train_target, train_predicted)\n        test_accuracy = accuracy_score(test_target, test_predicted)\n        train_precision = precision_score(train_target, train_predicted, average='weighted')\n        test_precision = precision_score(test_target, test_predicted, average='weighted')\n        train_recall = recall_score(train_target, train_predicted, average='weighted')\n        test_recall = recall_score(test_target, test_predicted, average='weighted')\n        train_f1score = f1_score(train_target, train_predicted, average='weighted')\n        test_f1score = f1_score(test_target, test_predicted, average='weighted')\n\n        train_accuracies.append(train_accuracy)\n        test_accuracies.append(test_accuracy)\n        train_precisions.append(train_precision)\n        test_precisions.append(test_precision)\n        train_recalls.append(train_recall)\n        test_recalls.append(test_recall)\n        train_f1scores.append(train_f1score)\n        test_f1scores.append(test_f1score)\n\n        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f},\"\n         f' Train Accuracy: {train_accuracy* 100:.2f}%, Test Accuracy: {test_accuracy* 100:.2f}%'\n             f\"Train Recall: {train_recall*100:.2f}%, Test Recall: {test_recall*100:2f}%\")\n\n    metrics = {\n        'train_losses': train_losses,\n        'test_losses': test_losses,\n        'train_accuracies': train_accuracies,\n        'test_accuracies': test_accuracies,\n        'train_predicted': train_predicted,\n        'train_target': train_target,\n        'test_predicted': test_predicted,\n        'test_target': test_target,\n        'train_precision': train_precisions,\n        'test_precision': test_precisions,\n        'train_recall': train_recalls,\n        'test_recall':test_recalls,\n        'train_f1score': train_f1score,\n        'test_f1score': test_f1score\n    }\n\n    return metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = [{}, {}, {}, {}]\nlr = [#0.001,\n      #0.0001,\n      0.0005,\n      0.00005]\noptimezers = ['Adam']\nmodels = {\n    0: \"ResNet-50\",\n    1: \"MobileNetV1\",\n    2: \"AlexNet\",\n    3: \"GoogleNet\"\n}\n\nfor optimizer_ in optimezers:\n    for learning_rate in lr:\n        if optimizer_ == 'Adam':            \n            optimizer = optim.Adam(r_cnn.parameters(), lr=learning_rate)\n            optimizer2 = optim.Adam(pretrained_mobilenet.parameters(), lr=learning_rate)\n            optimizer3 = optim.Adam(alexnet_pretrained.parameters(), lr=learning_rate)\n            optimizer4 = optim.Adam(google_cnn.parameters(), lr=learning_rate)\n            criterion = nn.CrossEntropyLoss() \n            criterion2 = nn.CrossEntropyLoss()\n            criterion3 = nn.CrossEntropyLoss() \n            criterion4 = nn.CrossEntropyLoss()\n\n        elif optimizer_ == 'RMSProp':\n            optimizer = optim.RMSProp(r_cnn.parameters(), lr=learning_rate)\n            optimizer2 = optim.RMSProp(pretrained_mobilenet.parameters(), lr=learning_rate)\n            optimizer3 = optim.RMSProp(alexnet_pretrained.parameters(), lr=learning_rate)\n            optimizer4 = optim.RMSProp(google_cnn.parameters(), lr=learning_rate)\n            criterion = nn.CrossEntropyLoss() \n            criterion2 = nn.CrossEntropyLoss()\n            criterion3 = nn.CrossEntropyLoss() \n            criterion4 = nn.CrossEntropyLoss()\n            \n        elif optimizer_ == 'SGD':\n            optimizer = optim.SGD(r_cnn.parameters(), lr=learning_rate)\n            optimizer2 = optim.SGD(pretrained_mobilenet.parameters(), lr=learning_rate)\n            optimizer3 = optim.SGD(alexnet_pretrained.parameters(), lr=learning_rate)\n            optimizer4 = optim.SGD(google_cnn.parameters(), lr=learning_rate)\n            criterion = nn.CrossEntropyLoss() \n            criterion2 = nn.CrossEntropyLoss()\n            criterion3 = nn.CrossEntropyLoss() \n            criterion4 = nn.CrossEntropyLoss()\n        \n        metrics[0] = train_validate_model(r_cnn, trainloader, testloader, criterion, optimizer, 1, epoch)\n        metrics[1] = train_validate_model(pretrained_mobilenet, trainloader, testloader, criterion,optimizer,2, epoch)\n        metrics[2] = train_validate_model(alexnet_pretrained, trainloader, testloader, criterion, optimizer,3, epoch)\n        metrics[3] = train_validate_model(google_cnn, trainloader, testloader, criterion, optimizer,4, epoch)\n        \n        for i in range (4):\n            plt.figure(figsize=(20, 15))\n            plt.subplot(3, 2, 1)\n            plt.plot(metrics[i]['train_losses'], label='Train Loss')\n            plt.plot(metrics[i]['test_losses'], label='Test Loss')\n            plt.xlabel('Epoch')\n            plt.ylabel('Loss')\n            plt.title(f'{models[i]} Losses, LR={learning_rate}, Optimizer={optimizer_}')\n            plt.legend()\n\n            plt.subplot(3, 2, 2)\n            plt.plot(metrics[i]['test_accuracies'], label='Test Accuracy', color='b')\n            plt.plot(metrics[i]['train_accuracies'], label='Train Accuracy', color='r')\n            plt.xlabel('Epoch')\n            plt.ylabel('Accuracy')\n            plt.title(f'{models[i]} Accuracy, LR={learning_rate}, Optimizer={optimizer_}')\n            plt.legend()\n            \n            plt.subplot(3, 2, 3)  \n            plt.plot(metrics[i]['precisions'], label='Precision', color='g')\n            plt.xlabel('Epoch')\n            plt.ylabel('Precision, ')\n            plt.title(f'{models[i]} Precision, LR={learning_rate}, Optimizer={optimizer_}')\n            plt.legend()\n\n            plt.subplot(3, 2, 4) \n            plt.plot(metrics[i]['recalls'], label='Recall', color='m')\n            plt.xlabel('Epoch')\n            plt.ylabel('Recall')\n            plt.title(f'{models[i]} Recall, LR={learning_rate}, Optimizer={optimizer_}')\n            plt.legend()\n\n            plt.subplot(3, 2, 5) \n            plt.plot(metrics[i]['f1_scores'], label='F1 Score', color='y')\n            plt.xlabel('Epoch')\n            plt.ylabel('F1 Score')\n            plt.title(f'{models[i]} F1 Score, LR={learning_rate}, Optimizer={optimizer_}')\n            plt.legend()\n\n        plt.tight_layout()\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}